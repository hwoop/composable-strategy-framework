# AdamW 옵티마이저의 학습률(learning rate)
lr: 2e-5

# 가중치 감쇠(weight decay) 값
# L2 정규화와 유사한 효과를 내며, 과적합을 방지하는 데 도움을 줍니다.
weight_decay: 0.01

# StepLR 스케줄러가 학습률을 감소시킬 epoch 간격
# 예: 5이면, 5 epoch마다 학습률이 변경
lr_step_size: 5

# StepLR 스케줄러가 학습률을 감소시킬 비율
# 예: 0.5이면, lr_step_size epoch마다 학습률이 이전의 0.5배가 됩니다.
lr_gamma: 0.5